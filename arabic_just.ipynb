{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torchaudio\n",
        "\n",
        "def is_mp3_valid(file_path):\n",
        "    try:\n",
        "        waveform, sample = torchaudio.load(file_path)\n",
        "        # print(sample)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "file_path_eng = '/content/drive/MyDrive/dataset/arabic/ar_en_asr_train_colab.csv'\n",
        "output_file_path = '/content/drive/MyDrive/dataset/arabic/ar_en_asr_train_colab_corr.csv'\n",
        "\n",
        "with open(file_path_eng, 'r', encoding='utf-8') as tsv_file, open(output_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        if row:\n",
        "            mp3_path = row[0]\n",
        "            if is_mp3_valid(mp3_path):\n",
        "                # Write the valid row to the new CSV file\n",
        "                csv_writer.writerow(row)"
      ],
      "metadata": {
        "id": "cZN4zI2Sq7jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "input_tsv_file = '/content/drive/MyDrive/dataset/arabic/train.tsv'\n",
        "output_csv_file = '/content/drive/MyDrive/dataset/arabic/train_colab.csv'\n",
        "\n",
        "# Open TSV file for reading and CSV file for writing\n",
        "with open(input_tsv_file, 'r', encoding='utf-8') as tsv_file, open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header to CSV file\n",
        "    csv_writer.writerow(['audio', 'transcript'])\n",
        "\n",
        "    # Skip the header in TSV file\n",
        "    next(tsv_reader)\n",
        "\n",
        "    # Process each row in TSV file and write to CSV file\n",
        "    for row in tsv_reader:\n",
        "        # print(row[0])\n",
        "        audio_path = '/content/drive/MyDrive/dataset/arabic/clips/' + row[1]\n",
        "        transcript = row[2]\n",
        "        csv_writer.writerow([audio_path, transcript])"
      ],
      "metadata": {
        "id": "JhdKq6CIapib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k_HJashXyKI",
        "outputId": "726c6893-c607-4902-f432-e6cac30e4891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezrKwTg1Xx85",
        "outputId": "39b5f4b9-9aa6-4fe5-ba3b-4290a8c49597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "Dl5uhF-GXn7P",
        "outputId": "4d56c385-bd42-49c9-c1e2-09dd9def8f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN set size: 2029\n",
            "TEST set size: 1695\n",
            "Num Model Parameters 27210504\n",
            "2029\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-9fb481c76fb0>:743: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  x, sample_rate = librosa.load(x)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__soundfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Otherwise, create the soundfile object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    657\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLibsndfileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error opening {0!r}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-9fb481c76fb0>\u001b[0m in \u001b[0;36m<cell line: 1027>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0mlibri_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test-other\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInfo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-9fb481c76fb0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, epochs, train_url, test_url)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         tra_loss, infoloss = train(model, premodel, device, train_loader, train_loader2, criterion, optimizer, preoptimizer, scheduler,\n\u001b[0m\u001b[1;32m    992\u001b[0m                                      prescheduler, epoch, gam, optimizer1, preoptimizer1)\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-9fb481c76fb0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, premodel, device, train_loader, train_loader2, criterion, optimizer, preoptimizer, scheduler, prescheduler, epoch, gam, optimizer1, preoptimizer1)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0mdata_len2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-9fb481c76fb0>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \"\"\"\n\u001b[1;32m    742\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 )\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-157>\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36m__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Would be 2, but the decorator adds a level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# If the input was not an audioread object, try to open it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# from ctcdecode import CTCBeamDecoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torchmetrics\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch.nn import MultiheadAttention\n",
        "import pandas as pd\n",
        "\n",
        "class InfoNCE(nn.Module):\n",
        "\n",
        "    def __init__(self, temperature=0.1, reduction='mean', negative_mode='unpaired'):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.reduction = reduction\n",
        "        self.negative_mode = negative_mode\n",
        "\n",
        "    def forward(self, query, positive_key, negative_keys=None):\n",
        "        return info_nce(query, positive_key, negative_keys,\n",
        "                        temperature=self.temperature,\n",
        "                        reduction=self.reduction,\n",
        "                        negative_mode=self.negative_mode)\n",
        "\n",
        "\n",
        "def info_nce(query, positive_key, negative_keys=None, temperature=0.1, reduction='mean', negative_mode='unpaired'):\n",
        "    # Check input dimensionality.\n",
        "    if query.dim() != 2:\n",
        "        raise ValueError('<query> must have 2 dimensions.')\n",
        "    if positive_key.dim() != 2:\n",
        "        raise ValueError('<positive_key> must have 2 dimensions.')\n",
        "    if negative_keys is not None:\n",
        "        if negative_mode == 'unpaired' and negative_keys.dim() != 2:\n",
        "            raise ValueError(\"<negative_keys> must have 2 dimensions if <negative_mode> == 'unpaired'.\")\n",
        "        if negative_mode == 'paired' and negative_keys.dim() != 3:\n",
        "            raise ValueError(\"<negative_keys> must have 3 dimensions if <negative_mode> == 'paired'.\")\n",
        "\n",
        "    # Check matching number of samples.\n",
        "    if len(query) != len(positive_key):\n",
        "        raise ValueError('<query> and <positive_key> must must have the same number of samples.')\n",
        "    if negative_keys is not None:\n",
        "        if negative_mode == 'paired' and len(query) != len(negative_keys):\n",
        "            raise ValueError(\"If negative_mode == 'paired', then <negative_keys> must have the same number of samples as <query>.\")\n",
        "\n",
        "    # Embedding vectors should have same number of components.\n",
        "    if query.shape[-1] != positive_key.shape[-1]:\n",
        "        raise ValueError('Vectors of <query> and <positive_key> should have the same number of components.')\n",
        "    if negative_keys is not None:\n",
        "        if query.shape[-1] != negative_keys.shape[-1]:\n",
        "            raise ValueError('Vectors of <query> and <negative_keys> should have the same number of components.')\n",
        "\n",
        "    # Normalize to unit vectors\n",
        "    query, positive_key, negative_keys = normalize(query, positive_key, negative_keys)\n",
        "    if negative_keys is not None:\n",
        "        # Explicit negative keys\n",
        "\n",
        "        # Cosine between positive pairs\n",
        "        positive_logit = torch.sum(query * positive_key, dim=1, keepdim=True)\n",
        "\n",
        "        if negative_mode == 'unpaired':\n",
        "            # Cosine between all query-negative combinations\n",
        "            negative_logits = query @ transpose(negative_keys)\n",
        "\n",
        "        elif negative_mode == 'paired':\n",
        "            query = query.unsqueeze(1)\n",
        "            negative_logits = query @ transpose(negative_keys)\n",
        "            negative_logits = negative_logits.squeeze(1)\n",
        "\n",
        "        # First index in last dimension are the positive samples\n",
        "        logits = torch.cat([positive_logit, negative_logits], dim=1)\n",
        "        labels = torch.zeros(len(logits), dtype=torch.long, device=query.device)\n",
        "    else:\n",
        "        # Negative keys are implicitly off-diagonal positive keys.\n",
        "\n",
        "        # Cosine between all combinations\n",
        "        logits = query @ transpose(positive_key)\n",
        "\n",
        "        # Positive keys are the entries on the diagonal\n",
        "        labels = torch.arange(len(query), device=query.device)\n",
        "\n",
        "    return F.cross_entropy(logits / temperature, labels, reduction=reduction)\n",
        "\n",
        "\n",
        "def transpose(x):\n",
        "    return x.transpose(-2, -1)\n",
        "\n",
        "\n",
        "def normalize(*xs):\n",
        "    return [None if x is None else F.normalize(x, dim=-1) for x in xs]\n",
        "\n",
        "\n",
        "\n",
        "class LibriSpeechDataset(Dataset):\n",
        "    def __init__(self, audio_files, waveform_length, context_length, future_length, negative_waveform_length):\n",
        "        self.audio_files = audio_files\n",
        "        self.waveform_length = waveform_length\n",
        "        self.context_length = context_length\n",
        "        self.future_length = future_length\n",
        "        self.negative_waveform_length = negative_waveform_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def load_waveform(self, audio_path, waveform_length):\n",
        "        waveform, _ = torchaudio.load(audio_path)\n",
        "        if waveform.size(1) > waveform_length:\n",
        "            start_idx = random.randint(0, waveform.size(1) - waveform_length)\n",
        "            waveform = waveform[:, start_idx: start_idx + waveform_length]\n",
        "        else:\n",
        "            pad_length = waveform_length - waveform.size(1)\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_length))\n",
        "        return waveform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_files[idx]\n",
        "        waveform = self.load_waveform(audio_path, self.waveform_length)\n",
        "\n",
        "        # Generate context waves\n",
        "        start_idx = random.randint(0, self.waveform_length - self.context_length - self.future_length)\n",
        "        context = waveform[:, start_idx: start_idx + self.context_length]\n",
        "\n",
        "        # Generate future samples\n",
        "        future = waveform[:, start_idx + self.context_length: start_idx + self.context_length + self.future_length]\n",
        "\n",
        "        # Generate negative sample\n",
        "        negative_idx = random.randint(0, len(self.audio_files) - 1)\n",
        "        while negative_idx == idx:\n",
        "            negative_idx = random.randint(0, len(self.audio_files) - 1)\n",
        "\n",
        "        negative_audio_path = self.audio_files[negative_idx]\n",
        "        negative_waveform = self.load_waveform(negative_audio_path, self.negative_waveform_length)\n",
        "\n",
        "        negative_sample = negative_waveform\n",
        "\n",
        "        # Return context, future, negative sample, and waveform length\n",
        "        return context, future, negative_sample, context.size(1)\n",
        "\n",
        "\n",
        "\n",
        "class preCNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(preCNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "#         print(x.size())\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "\n",
        "class preResidualCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(preResidualCNN, self).__init__()\n",
        "\n",
        "        self.self_attention1 = MultiheadAttention(embed_dim=in_channels, num_heads=8)\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels*2, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels*2, out_channels*2, kernel, stride, padding=kernel//2)\n",
        "        self.cnn3 = nn.Conv2d(out_channels*2, out_channels*2, kernel, stride, padding=kernel//2)\n",
        "        self.cnn4 = nn.Conv2d(out_channels*2, out_channels, kernel, stride, padding=kernel//2)\n",
        "\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.dropout4 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = preCNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = preCNNLayerNorm(n_feats)\n",
        "        self.layer_norm3 = preCNNLayerNorm(n_feats)\n",
        "        self.layer_norm4 = preCNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "\n",
        "        sizes = x.size()\n",
        "\n",
        "        x = x.view(sizes[0], sizes[1], sizes[2]*sizes[3])\n",
        "        x = x.transpose(1,2)\n",
        "\n",
        "        x, _ = self.self_attention1(x, x, x)\n",
        "        x = x.transpose(1,2)\n",
        "        x = x.view(sizes)\n",
        "\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "\n",
        "\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "\n",
        "\n",
        "        x = self.layer_norm3(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x = self.dropout3(x)\n",
        "        x = self.cnn3(x)\n",
        "\n",
        "\n",
        "        x = self.layer_norm4(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x = self.dropout4(x)\n",
        "        x = self.cnn4(x)\n",
        "\n",
        "\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class preBidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(preBidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class preSpeechRecognitionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1, n_predictions=5):\n",
        "        super(preSpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.n_predictions = n_predictions\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            preResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            preBidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim*2),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim*2, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "#         print(f'input x: {x.size()}')\n",
        "\n",
        "\n",
        "#         print(x.size())\n",
        "        x = self.cnn(x)\n",
        "#         print(f'output of cnn: {x.size()}')\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x[:, -1:, :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.self_attention1 = MultiheadAttention(embed_dim=in_channels, num_heads=8)\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels*2, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels*2, out_channels*2, kernel, stride, padding=kernel//2)\n",
        "        self.cnn3 = nn.Conv2d(out_channels*2, out_channels*2, kernel, stride, padding=kernel//2)\n",
        "        self.cnn4 = nn.Conv2d(out_channels*2, out_channels, kernel, stride, padding=kernel//2)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.dropout4 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm3 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm4 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1], sizes[2]*sizes[3])\n",
        "        x = x.transpose(1,2)\n",
        "        x, _ = self.self_attention1(x, x, x)\n",
        "        x = x.transpose(1,2)\n",
        "        x = x.view(sizes)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "\n",
        "\n",
        "        x = self.layer_norm3(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x = self.dropout3(x)\n",
        "        x = self.cnn3(x)\n",
        "\n",
        "\n",
        "        x = self.layer_norm4(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        x = self.dropout4(x)\n",
        "        x = self.cnn4(x)\n",
        "\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim*4),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim*4, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "class SentencePieceTransform:\n",
        "    \"\"\"Maps subwords to integers and vice versa using SentencePiece\"\"\"\n",
        "    def __init__(self, model_path):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(model_path)\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use the SentencePiece tokenizer to convert text to an integer sequence \"\"\"\n",
        "        subwords = self.sp.EncodeAsPieces(text.lower())\n",
        "        return [self.sp.PieceToId(subword) for subword in subwords]\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use the SentencePiece tokenizer to convert integer labels to a text sequence \"\"\"\n",
        "\n",
        "        return self.sp.decode(labels)\n",
        "\n",
        "sentencepiece_transform = SentencePieceTransform(\"/content/drive/MyDrive/dataset/data/spm_unigram_1023.model\")\n",
        "\n",
        "\n",
        "def get_audio_transforms():\n",
        "    time_masks = [torchaudio.transforms.TimeMasking(time_mask_param=15, p=0.05) for _ in range(10)]\n",
        "    train_audio_transform = nn.Sequential(\n",
        "        torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80, hop_length=160),\n",
        "        torchaudio.transforms.FrequencyMasking(freq_mask_param=27),\n",
        "        *time_masks,\n",
        "    )\n",
        "    valid_audio_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80, hop_length=160)\n",
        "    return train_audio_transform, valid_audio_transform\n",
        "\n",
        "train_audio_transforms, valid_audio_transforms = get_audio_transforms()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(sentencepiece_transform.text_to_int(utterance))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "def numtoword(beam_results, out_lens, labels, label_lengths,blank_label=0, collapse_repeated=True):\n",
        "    arg_maxes = beam_results\n",
        "\n",
        "    decodes = []\n",
        "    targets = []\n",
        "\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        tar_list = labels[i][:label_lengths[i]].tolist()\n",
        "        tar_list = list(map(int, tar_list))\n",
        "        tar_list = list(filter(lambda x: x != 0, tar_list))\n",
        "        targets.append(sentencepiece_transform.int_to_text(tar_list))\n",
        "\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j-1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(sentencepiece_transform.int_to_text(decode))\n",
        "    return decodes, targets\n",
        "\n",
        "\n",
        "def loss_F(parameters):\n",
        "    return sum(torch.linalg.norm(w) ** 2 for w in parameters)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_fn = InfoNCE()\n",
        "\n",
        "def train(model, premodel, device, train_loader, train_loader2, criterion, optimizer, preoptimizer, scheduler,\n",
        "                                     prescheduler, epoch, gam, optimizer1, preoptimizer1):\n",
        "    model.train()\n",
        "    premodel.train()\n",
        "\n",
        "    train_loss = 0\n",
        "    info_loss = 0\n",
        "\n",
        "    data_len = len(train_loader.dataset)\n",
        "    data_len2 = len(train_loader2.dataset)\n",
        "\n",
        "    for batch_idx, (_data, predata) in enumerate(zip(train_loader, train_loader2)):\n",
        "\n",
        "            context, future, negative_samples, lengths = predata\n",
        "            context = context.to(device)\n",
        "            future = future.to(device)\n",
        "            negative_samples = negative_samples.to(device)\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            context = context.unsqueeze(1)\n",
        "            context = context.repeat(1, 1, 80, 1)\n",
        "\n",
        "            predictions = premodel(context)\n",
        "\n",
        "\n",
        "            sizes = predictions.size()\n",
        "\n",
        "\n",
        "            predictions = predictions.view(sizes[0], sizes[1]*sizes[2])\n",
        "\n",
        "            target = future.view(sizes[0], sizes[1]*sizes[2])\n",
        "\n",
        "            neg_target = negative_samples.view(sizes[0], sizes[1]*sizes[2])\n",
        "\n",
        "            lamda = .001\n",
        "\n",
        "            reg =  loss_F(premodel.parameters())\n",
        "\n",
        "            loss_cpc = loss_fn(predictions, target, neg_target) + lamda*reg  # gxy\n",
        "\n",
        "            if batch_idx % 400 == 0 or batch_idx == data_len2:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tCPC_Loss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(context), data_len2,\n",
        "                    100. * batch_idx / len(train_loader2), loss_cpc.item()))\n",
        "\n",
        "\n",
        "        ######################Supervised training portion#########################################\n",
        "\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "\n",
        "            gam = round(gam, 3)\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "\n",
        "\n",
        "            # loss_nce = criterion(output, labels, input_lengths, label_lengths) + gam*loss_cpc  #(fy + gam* (gxy-vx))\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "#            for name, param in model.named_parameters():\n",
        "#                if name != 'last_layer_weight' and name != 'last_layer_bias':\n",
        "#                          param.requires_grad = True\n",
        "#\n",
        "#                else:\n",
        "#                  param.requires_grad = False\n",
        "\n",
        "\n",
        "            # preoptimizer.zero_grad()\n",
        "            # optimizer1.zero_grad()\n",
        "\n",
        "            # loss_nce.backward(retain_graph=True)\n",
        "#             torch.nn.utils.clip_grad_norm_(parameters=premodel.parameters(), max_norm=10, norm_type=2.0)\n",
        "#             torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=10, norm_type=2.0)\n",
        "            # preoptimizer.step()\n",
        "            # optimizer1.step()\n",
        "\n",
        "\n",
        "            # optimizer.zero_grad()\n",
        "\n",
        "#            for name, param in model.named_parameters():\n",
        "#                if name != 'last_layer_weight' and name != 'last_layer_bias':\n",
        "#                  param.requires_grad = False\n",
        "#\n",
        "#                else:\n",
        "#                  param.requires_grad = True\n",
        "\n",
        "            ctc_loss = criterion(output, labels, input_lengths, label_lengths)+gam*loss_cpc\n",
        "\n",
        "            ctc_loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += ctc_loss.item() / len(train_loader)\n",
        "\n",
        "\n",
        "            if batch_idx % 400 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tCTC_Loss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), ctc_loss.item()))\n",
        "\n",
        "                print(f'gamma: {gam}')\n",
        "    print(f'train_loss: {train_loss}')\n",
        "    scheduler.step()\n",
        "    prescheduler.step()\n",
        "\n",
        "\n",
        "    return train_loss, info_loss\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    n_classes = 1000\n",
        "\n",
        "    if epoch%2000==0:\n",
        "        with torch.no_grad():\n",
        "                for i, _data in enumerate(test_loader):\n",
        "                    spectrograms, labels, input_lengths, label_lengths = _data\n",
        "\n",
        "                    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                    output = model(spectrograms)  # (batch, time, n_class)\n",
        "                    soft_max = torch.nn.functional.softmax(output,dim=2)\n",
        "                    output_lengths = torch.full((output.size(0),), output.size(1), dtype=torch.int32)\n",
        "                    output = F.log_softmax(output, dim=2)\n",
        "                    output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "                    loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                    test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "                    itera = spectrograms.size()\n",
        "\n",
        "    #                 print(\"output for greedy\")\n",
        "\n",
        "    #                 decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "    #                 print(f'predtion: {decoded_preds1}')\n",
        "    #                 print(f'Labels: {decoded_targets1}')\n",
        "                    decoder = CTCBeamDecoder(\n",
        "                        [''] * (n_classes - 1) + [' '],\n",
        "                        model_path=None,\n",
        "                        alpha=0,\n",
        "                        beta=0,\n",
        "                        cutoff_top_n=40,\n",
        "                        cutoff_prob=1.0,\n",
        "                        beam_width=1000,\n",
        "                        num_processes=4,\n",
        "                        blank_id=0,\n",
        "                        log_probs_input=False\n",
        "                    )\n",
        "                    beam_results, beam_scores, timesteps, out_lens = decoder.decode(soft_max, output_lengths)\n",
        "                    b=[]\n",
        "                    for i in range(itera[0]):\n",
        "                         b.append(beam_results[i][0][:out_lens[i][0]])\n",
        "                    decoded_preds, decoded_targets = numtoword(b,out_lens,labels, label_lengths)\n",
        "\n",
        "                    for j in range(len(decoded_preds)):\n",
        "                        test_cer.append(torchmetrics.functional.char_error_rate(decoded_targets[j], decoded_preds[j]))\n",
        "                        test_wer.append(torchaudio.functional.edit_distance(decoded_targets[j], decoded_preds[j]) / len(\n",
        "    decoded_targets[j]\n",
        "))\n",
        "\n",
        "        avg_cer = sum(test_cer)/len(test_cer)\n",
        "        avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "#         model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "\n",
        "        print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "        # file_path = \"/home/exx/Desktop/saif/conformer/wer.txt\"\n",
        "        # with open(file_path, \"a\") as file:\n",
        "        #     file.write(f\"Epoch {epoch}: {avg_wer}\\n\")\n",
        "\n",
        "        return test_loss, avg_cer, avg_wer\n",
        "    #     return beam_results, out_lens, output\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data\n",
        "\n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                soft_max = torch.nn.functional.softmax(output,dim=2)\n",
        "                output_lengths = torch.full((output.size(0),), output.size(1), dtype=torch.int32)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                test_loss += loss.item() / len(test_loader)\n",
        "            print('Test set: Average loss: {:.4f}\\n'.format(test_loss))\n",
        "\n",
        "        return test_loss, 0 , 0\n",
        "\n",
        "\n",
        "class ASR(Dataset):\n",
        "    \"\"\"\n",
        "    Stores a Pandas DataFrame in __init__, and reads and preprocesses examples in __getitem__.\n",
        "    \"\"\"\n",
        "    def __init__(self, split, path, augmentation):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            augmentation (bool): Apply SpecAugment to training data or not.\n",
        "        \"\"\"\n",
        "        if split.upper()=='TRAIN':\n",
        "            file_path = path\n",
        "            self.df1 = pd.read_csv(file_path)\n",
        "\n",
        "            self.df = pd.concat([self.df1], ignore_index=True)\n",
        "\n",
        "\n",
        "        # self.df = pd.read_csv('%s.csv' % split.upper())\n",
        "        # self.tokenizer = torch.load('tokenizer.pth')\n",
        "\n",
        "        if split.upper()=='TEST':\n",
        "            self.df = pd.read_csv(path)\n",
        "        self.augmentation = (augmentation and (split.upper() == 'TRAIN'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            x (torch.FloatTensor, [seq_length, dim_features]): The FBANK features.\n",
        "            y (torch.LongTensor, [n_tokens]): The label sequence.\n",
        "        \"\"\"\n",
        "        x, y = self.df.iloc[idx]\n",
        "        x, sample_rate = librosa.load(x)\n",
        "\n",
        "\n",
        "        return x, y\n",
        "\n",
        "def data_processing_c(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, utterance) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "\n",
        "\n",
        "        elif data_type == 'test' or \"valid\":\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(sentencepiece_transform_c.text_to_int(utterance))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0])\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    #print(spectrograms.size())\n",
        "#\n",
        "#    print(labels.size())\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def load(split, path, batch_size, workers=0, augmentation=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        split (string): Which of the subset of data to take. One of 'train', 'dev' or 'test'.\n",
        "        batch_size (integer): Batch size.\n",
        "        workers (integer): How many subprocesses to use for data loading.\n",
        "        augmentation (bool): Apply SpecAugment to training data or not.\n",
        "\n",
        "    Returns:\n",
        "        loader (DataLoader): A DataLoader can generate batches of (FBANK features, FBANK lengths, label sequence).\n",
        "    \"\"\"\n",
        "    assert split in ['train', 'dev', 'test']\n",
        "\n",
        "    dataset = ASR(split, path, augmentation)\n",
        "    # print(dataset)\n",
        "    print (\"%s set size:\"%split.upper(), len(dataset))\n",
        "\n",
        "    # kwargs = {'num_workers': 6, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    loader = DataLoader(dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        collate_fn=lambda x: data_processing_c(x, split),\n",
        "                        num_workers=workers,\n",
        "                        pin_memory=True)\n",
        "    return loader\n",
        "\n",
        "def get_audio_files_wav(data_dir):\n",
        "    return [os.path.join(root, file) for root, dirs, files in os.walk(data_dir) for file in files if file.lower().endswith('.mp3')]\n",
        "\n",
        "\n",
        "\n",
        "sentencepiece_transform_c = SentencePieceTransform(\"/content/drive/MyDrive/dataset/arabic/arabic_unigram1000_model.model\")\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=10, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
        "\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3, #### 2,3,4\n",
        "        \"n_rnn_layers\": 5, ##### 4,5,6\n",
        "        \"rnn_dim\": 512, #512,\n",
        "        \"n_class\": 1000,\n",
        "        \"n_feats\": 80,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.05,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # if not os.path.isdir(\"/content/drive/MyDrive/dataset/data/data100\"):\n",
        "    #     os.makedirs(\"/content/drive/MyDrive/dataset/data/data100\")\n",
        "\n",
        "    # train_dataset = torchaudio.datasets.LIBRISPEECH(\"/content/drive/MyDrive/dataset/data/data100\", url=train_url, download=True)\n",
        "    # test_dataset = torchaudio.datasets.LIBRISPEECH(\"/content/drive/MyDrive/dataset/data/data100\", url=test_url, download=True)\n",
        "\n",
        "\n",
        "    path_train_e = \"/content/drive/MyDrive/dataset/arabic/train_colab.csv\"\n",
        "    path_test_e = \"/content/drive/MyDrive/dataset/arabic/ar_en_asr_test_colab.csv\"\n",
        "\n",
        "    train_loader = load('train', path_train_e, 10)\n",
        "    test_loader = load('test', path_test_e, 10)\n",
        "\n",
        "\n",
        "\n",
        "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
        "    # train_loader = data.DataLoader(dataset=train_dataset,#combined_dataset,\n",
        "    #                             batch_size=10,#hparams['batch_size'],\n",
        "    #                             shuffle=True,\n",
        "    #                             collate_fn=lambda x: data_processing(x, 'train'),\n",
        "    #                             **kwargs)\n",
        "    # test_loader = data.DataLoader(dataset=test_dataset,\n",
        "    #                             batch_size=hparams['batch_size'],\n",
        "    #                             shuffle=False,\n",
        "    #                             collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "    #                             **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        )\n",
        "\n",
        "    model.to(device)\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "#     print(model)\n",
        "\n",
        "\n",
        "\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=1, zero_infinity=True).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    optimizer1 = optim.AdamW(model.parameters(), lr=5e-4) ###. fine-tuning learning rate\n",
        "\n",
        "\n",
        "\n",
        "    ####################################Pre training######################################\n",
        "\n",
        "    # if not os.path.isdir(\"/content/drive/MyDrive/dataset/data/data100\"):\n",
        "    #     os.makedirs(\"/content/drive/MyDrive/dataset/data/data100\")\n",
        "\n",
        "\n",
        "    data_dir = \"/content/drive/MyDrive/dataset/arabic/clips\" # pre-training data path.\n",
        "\n",
        "\n",
        "    audio_files = get_audio_files_wav(data_dir)\n",
        "\n",
        "    waveform_length = 32000  # Length of the waveform (can be adjusted as needed)\n",
        "    context_length = 256  # Length of the context wave\n",
        "    future_length = 12  # Length of the future samples\n",
        "    negative_waveform_length = 12\n",
        "\n",
        "\n",
        "    train_dataset2 = LibriSpeechDataset(audio_files, waveform_length, context_length, future_length, negative_waveform_length)\n",
        "      # Adjust the batch size as needed\n",
        "    train_loader2 = DataLoader(train_dataset2, batch_size=hparams['batch_size']) # Iterate over the data loader\n",
        "\n",
        "    print(len(train_loader.dataset))\n",
        "    print(len(train_loader2.dataset))\n",
        "\n",
        "    prehparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512, #512,\n",
        "        \"n_class\": 12,\n",
        "        \"n_feats\": 80, # do not change\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.05,\n",
        "        \"n_predictions\": 5,\n",
        "        \"epochs\": 10\n",
        "            }\n",
        "\n",
        "\n",
        "    premodel = preSpeechRecognitionModel(\n",
        "            prehparams['n_cnn_layers'], prehparams['n_rnn_layers'], prehparams['rnn_dim'],\n",
        "            prehparams['n_class'], prehparams['n_feats'], prehparams['stride'], prehparams['dropout'], prehparams['n_predictions']\n",
        "            )\n",
        "\n",
        "\n",
        "#     preoptimizer = optim.AdamW(premodel.parameters(), lr=0.001)\n",
        "\n",
        "    preoptimizer = optim.AdamW(premodel.parameters(), lr=5e-3) # Pre-training learniong rate\n",
        "\n",
        "\n",
        "    preoptimizer1 = optim.AdamW(premodel.parameters(), lr=hparams['learning_rate'])\n",
        "#     prescheduler1 = optim.lr_scheduler.OneCycleLR(preoptimizer, max_lr=.001,\n",
        "#                                             steps_per_epoch=int(len(train_loader2)),\n",
        "#                                             epochs=hparams['epochs'],\n",
        "#                                             anneal_strategy='linear')\n",
        "\n",
        "    prescheduler = optim.lr_scheduler.OneCycleLR(preoptimizer1, max_lr=.001,\n",
        "                                            steps_per_epoch=int(len(train_loader2)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "\n",
        "    premodel.to(device)\n",
        "    premodel = nn.DataParallel(premodel)\n",
        "\n",
        "\n",
        "\n",
        "#     model = SpeechRecognitionModel(premodel,latent_dim=1024, num_classes= hparams[\"n_class\"])\n",
        "\n",
        "#     model = nn.DataParallel(model)\n",
        "\n",
        "#     print(model)\n",
        "\n",
        "#     model.to(device)\n",
        "\n",
        "#     print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "#     optimizer = optim.AdamW(model.parameters(), lr=hparams['learning_rate'])\n",
        "#     criterion = nn.CTCLoss(blank=0).to(device)\n",
        "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "#                                             steps_per_epoch=int(len(train_loader)),\n",
        "#                                             epochs=hparams['epochs'],\n",
        "#                                             anneal_strategy='linear')\n",
        "\n",
        "\n",
        "\n",
        "    gamma_max = 1\n",
        "    gamma_init = 0\n",
        "    gamma_argmax_step = 500\n",
        "    if gamma_init > gamma_max:\n",
        "        gamma_max = gamma_init\n",
        "        print('Initial gamma is larger than max gamma, proceeding with gamma_max=gamma_init.')\n",
        "    gam = gamma_init\n",
        "    step_gam = (gamma_max-gamma_init)/gamma_argmax_step\n",
        "\n",
        "\n",
        "    train_loss=[]\n",
        "    test_loss=[]\n",
        "    Info_loss = []\n",
        "    cer=[]\n",
        "    wer=[]\n",
        "\n",
        "    tes_loss1=6\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "\n",
        "        tra_loss, infoloss = train(model, premodel, device, train_loader, train_loader2, criterion, optimizer, preoptimizer, scheduler,\n",
        "                                     prescheduler, epoch, gam, optimizer1, preoptimizer1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        gam+= step_gam\n",
        "\n",
        "        gam = min(gamma_max,gam)\n",
        "\n",
        "        tes_loss, c, w =  test(model, device, test_loader, criterion, epoch)\n",
        "\n",
        "        # if tes_loss<tes_loss1:\n",
        "        #     tes_loss1=tes_loss\n",
        "        #     torch.save(model.state_dict(), '/home/exx/Desktop/saif/conformer/lstm100model.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         scheduler.step(tes_loss)\n",
        "        train_loss.append(tra_loss)\n",
        "        test_loss.append(tes_loss)\n",
        "        Info_loss.append(infoloss)\n",
        "        cer.append(c)\n",
        "        wer.append(w)\n",
        "#         if w<best_wer:\n",
        "#             best_wer = w\n",
        "    return train_loss, test_loss, cer, wer, Info_loss\n",
        "\n",
        "\n",
        "learning_rate = 5e-4\n",
        "batch_size = 10\n",
        "epochs = 100\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-other\"\n",
        "\n",
        "train_loss, test_loss, cer, wer, Info_loss = main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install librosa soundfile\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Load the audio file\n",
        "file_path = \"/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3\"\n",
        "y, sr = librosa.load(file_path)\n",
        "\n",
        "# Play the audio\n",
        "ipd.display(ipd.Audio(y, rate=sr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "on6iFsKv2DC2",
        "outputId": "f6a4e5f3-7492-4e64-c339-f569e6fbbc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-b49e0606881e>:9: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(file_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__soundfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Otherwise, create the soundfile object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    657\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLibsndfileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error opening {0!r}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-b49e0606881e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Play the audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 )\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-157>\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36m__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Would be 2, but the decorator adds a level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# If the input was not an audioread object, try to open it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/arabic/clips/common_voice_ar_19227720.mp3'"
          ]
        }
      ]
    }
  ]
}